{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "168f6a0a-f1b0-47cd-9b2d-bc3257210301",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca8a54-96b3-4c91-b266-c8960885d6b3",
   "metadata": {},
   "source": [
    "# Multivariate Regression\n",
    "\n",
    "We have a given dataset \n",
    "\n",
    "$$\n",
    "    (x,y) \\in \\mathbb{R}^{m \\times (n+1)},\n",
    "$$\n",
    "\n",
    "where $m$ is the number of samples and $n$ is the number of features.\n",
    "\n",
    "In the dateset we have the features\n",
    "\n",
    "$$\n",
    "    x \\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "\n",
    "and the targets \n",
    "\n",
    "$$\n",
    "    y \\in \\mathbb{R}^m.\n",
    "$$\n",
    "\n",
    "We say that \n",
    "\n",
    "$$\n",
    "    (x^{(i)}, y^{(i)}) \\in \\mathbb{R}^{1 \\times (n+1)}\n",
    "$$\n",
    "\n",
    "is the $i$-th example ($i = 1, \\dots, m$) and \n",
    "\n",
    "$$\n",
    "    x_j \\in \\mathbb{R}^m\n",
    "$$ \n",
    "\n",
    "is the $j$-th feature vector ($j = 1, \\dots, n$), such that \n",
    "\n",
    "$$\n",
    "    x^{(i)}_j \\in \\mathbb{R}  \n",
    "$$\n",
    "\n",
    "is the $j$-th feature of the $i$-th example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf500d-29aa-4eaf-aed5-9e884dcf2d44",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a9ce5ce3-57c9-49bf-ad09-006a409ac76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "def plot_data(X, y):\n",
    "    \"\"\"Plot 3d regression prblem.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shape (n_samples, 2)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], y)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d187f75-0c63-413d-b323-4b369d3121d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4522fe0c1ea4d10b328e2176363846c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make simple example\n",
    "X, y = make_regression(n_samples=100, n_features=2, n_informative=2, noise=100,\n",
    "                       random_state=0)\n",
    "# Plot data\n",
    "plot_data(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d517d8e-31fc-441f-b226-0855d198fce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 2), (30, 2), (70,), (30,))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data in training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=True)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006e765-1843-49f1-a107-fa71500dae3e",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "The hypothesis is given by\n",
    "\n",
    "$$\n",
    "    h_{w, b}(X) = \\sum_{j=1}^n w_j X_j + b = w_1 X_1 + w_2 X_2 + \\dots + w_n X_n + b, \n",
    "$$\n",
    "\n",
    "where $w_1, w_2, \\dots, w_n \\in \\mathbb{R}^n$, with $n$ the number of features, are the weights and $b \\in \\mathbb{R}$ is the bias.\n",
    "\n",
    "We can also write\n",
    "\n",
    "$$\n",
    "    h_{w}(X) = \\sum_{j=0}^n w_j X_j = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_n X_n, \n",
    "$$\n",
    "\n",
    "if we set $w_0 := b$ and $X_0 := 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecc707d5-f618-4897-b25f-6d533e940026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the hypothesis and return as shape (n_samples,)\n",
    "hypo = lambda X, w, b: np.sum(w * X, axis=1) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811619a1-52fc-4096-b607-7afb9d824e55",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "The Mean Squared Error is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    MSE(w,b) &= \\frac{1}{m} \\sum\\limits_{i=1}^m (y^{(i)} - h_{w, b}(X^{(i)}))^2\\\\\n",
    "             &= \\frac{1}{m} \\sum\\limits_{i=1}^m (y^{(i)} - \\sum_{j=1}^n w_j X_j^{(i)} + b)^2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of samples and $n$ is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ce523-a217-4365-8998-479e68a01244",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "#### Goal: \n",
    "\n",
    "$$\n",
    "    \\underset{w \\in \\mathbb{R}, ~ b \\in \\mathbb{R}}{\\textbf{minimize}} MSE(w,b)\n",
    "$$\n",
    "\n",
    "#### Idea: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    w &= w - \\alpha \\cdot \\frac{\\partial}{\\partial w} MSE(w,b) \\\\\n",
    "    b &= b - \\alpha \\cdot \\frac{\\partial}{\\partial b} MSE(w,b)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Partial derivaties:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial w} MSE(w,b) &= \\frac{2}{m} \\sum\\limits_{i=1}^m x^{(i)} (y^{(i)} - h_{w,b}(X^{(i)})) \\\\\n",
    "    \\frac{\\partial}{\\partial b} MSE(w,b) &= \\frac{2}{m} \\sum\\limits_{i=1}^m y^{(i)} - h_{w,b}(X^{(i)})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6bdd0eda-738e-4243-8b63-8560c83b3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, b, alpha, num_iters):  \n",
    "    \"\"\"Simple gradient descent.\n",
    "    \n",
    "    TODO: Plot cost function.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.\n",
    "        w : float\n",
    "            Weight.\n",
    "        b : float\n",
    "            Bias.\n",
    "        alpha : float\n",
    "            Stepsize.\n",
    "        num_iter : int\n",
    "            Number of iterations.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        w : float\n",
    "            Updated weight.\n",
    "        b : float\n",
    "            Updated bias.\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    m = len(X)\n",
    "    \n",
    "    # Iteratively update the weight and bias\n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # Compute predictions (for all samples)\n",
    "        predictions = hypo(X, w, b)\n",
    "        \n",
    "        # Compute residuals (for all samples)\n",
    "        residuals = predictions - y\n",
    "        \n",
    "        # Compute partial derivitaves \n",
    "        w_gradient = 2 * np.mean(residuals.reshape(-1, 1) * X, axis=0)\n",
    "        b_gradient = 2 * np.mean(residuals)\n",
    "\n",
    "        # Update weight and bias\n",
    "        w = w - alpha * w_gradient\n",
    "        b = b - alpha * b_gradient\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "66da597c-0082-41cb-862d-84e526fa0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, alpha, num_iters):\n",
    "    \"\"\"Linear regression using gradient descent.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shpae (n_samples, n_features)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.        \n",
    "        alpha : float\n",
    "            Stepsize.\n",
    "        num_iter : int\n",
    "            Number of iterations.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        w : float\n",
    "            Updated weight.\n",
    "        b : float\n",
    "            Updated bias.\n",
    "    \"\"\"\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0.0\n",
    "    w, b = gradient_descent(X, y, w, b, alpha, num_iters)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c7529c69-8d4e-4d26-ab63-3f7fbdd8467d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8.86580537, 97.08947463]), -3.9879246863309175)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply linear regression with gradient descent\n",
    "w, b = linear_regression(X_train, y_train, 0.0001, 10000)\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf1b6054-e104-4c0e-a4bc-fe6e1e552e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_reg_plane(X, y, w, b, poly_degree=1):\n",
    "    \"\"\"Plot 3d regression problem and regression plane.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shape (n_samples, 2)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.\n",
    "        w : ndarray of shape (2,)\n",
    "            Weight.\n",
    "        b : float\n",
    "            Bias.\n",
    "        poly_degree : int, default=1\n",
    "            Polynom degree.\n",
    "    \"\"\"\n",
    "    fig, ax = plot_data(X, y)\n",
    "    xx = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n",
    "    yy = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)\n",
    "    xx, yy = np.meshgrid(xx, yy)\n",
    "    zz = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "    zz = PolynomialFeatures(poly_degree, include_bias=False).fit_transform(zz)\n",
    "    zz = hypo(zz, w, b)\n",
    "    ax.plot_surface(xx, yy, zz.reshape(xx.shape), color='r', alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5138b44a-0761-4994-9c92-0e367d20926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a391f0c75bb644cd8eee85aea6bab251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training set and regression plane\n",
    "plot_data_reg_plane(X_train, y_train, w, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c9625502-9efe-42c1-b535-17643a35a76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b098649a8b4442b98b15654323725ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot test set and regression plane\n",
    "plot_data_reg_plane(X_test, y_test, w, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "58b49138-5d22-4ae4-8d9d-6cbca34f48ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9739.291151365584, 8055.034961372016)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test MSE\n",
    "(mean_squared_error(y_train, hypo(X_train, w, b)),\n",
    " mean_squared_error(y_test, hypo(X_test, w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7308ee37-ad35-40ea-90f4-d01db238634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5714267338738095, 0.5551803374565513)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test R2\n",
    "r2_score(y_train, hypo(X_train, w, b)), r2_score(y_test, hypo(X_test, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b3471-87d8-4c93-9324-512edd78081e",
   "metadata": {},
   "source": [
    "### Normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "54c140e0-e47c-4f01-9ef0-1b4ab89a850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    \"\"\"Normal equation for linear regression.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "        theta : ndarray of shape (n_features + 1,)\n",
    "            Weight(s) and bias.\n",
    "    \"\"\"\n",
    "    X_hat = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    X_hat_trans = np.transpose(X_hat)\n",
    "    theta = np.linalg.inv(np.dot(X_hat_trans, X_hat))\n",
    "    theta = np.dot(np.dot(theta, X_hat_trans), y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "19d19c7d-930b-43cc-8c0b-27b03eecedf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  9.79488969, 109.8894111 ]), -7.211176719046206)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the normal equation\n",
    "theta = normal_equation(X_train, y_train)\n",
    "w, b = theta[1:], theta[0]\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1f190c52-35b2-418a-90b9-6b53f5fdd96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42456d5cd064d228f8dde3eb4dcb37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training set and regression plane\n",
    "plot_data_reg_plane(X_train, y_train, w, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ce27a0a-fb3d-45e8-a951-5bb680d0c698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c97bf04b68c4d2cb91f3e8e7b86277f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot test set and regression plane\n",
    "plot_data_reg_plane(X_test, y_test, w, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c5a7dda9-c9b9-4fcc-8983-8a3f18e84b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9556.393293716474, 8138.52818247132)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test MSE\n",
    "(mean_squared_error(y_train, hypo(X_train, w, b)),\n",
    " mean_squared_error(y_test, hypo(X_test, w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "89330213-7b70-43c1-b676-1f72e632b78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5794750744564987, 0.5505696279298813)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test R2\n",
    "r2_score(y_train, hypo(X_train, w, b)), r2_score(y_test, hypo(X_test, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2786dc-f167-4119-91ab-f63dbbbc5723",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "TODO: Update\n",
    "\n",
    "The hypothesis for (univariate) *polynomial regression* is given by\n",
    "\n",
    "$$\n",
    "    h_{w, b}(X) = \\sum_{i=1}^h w_i X_i^i + b =  w_1 X + w_2 X^2 + \\dots + w_h X^h + b, \n",
    "$$\n",
    "\n",
    "where $h$ is the degree of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ceda1b24-ddc5-4cdb-944a-2a4db54bac7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53924919, -0.76991607,  0.29078969, -0.41517662,  0.59277076],\n",
       "       [ 0.12167502,  0.76103773,  0.01480481,  0.09259928,  0.57917842],\n",
       "       [ 1.45427351,  0.14404357,  2.11491143,  0.20947875,  0.02074855],\n",
       "       [-0.43782004, -1.14746865,  0.19168639,  0.50238478,  1.31668431],\n",
       "       [-1.60205766,  0.62523145,  2.56658873, -1.00165683,  0.39091437]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(2, include_bias=False).fit(X_train)\n",
    "X_train = poly.transform(X_train)\n",
    "X_test = poly.transform(X_test)\n",
    "\n",
    "# Show some samples\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b1aa6159-6390-4265-b6f3-a5940ea01875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.82544018e+00, 1.01167739e+02, 5.09069654e-03, 2.30228501e+00,\n",
       "        2.43514531e+01]),\n",
       " -32.963612410449706)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train linear regression\n",
    "theta = normal_equation(X_train, y_train)\n",
    "w, b = theta[1:], theta[0]\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c73bdf27-0f4d-4404-9bda-cfe921686b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147992df27284978aff359e15f9dbe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training set and regression plane\n",
    "plot_data_reg_plane(X_train, y_train, w, b, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "37ab5f93-d7ef-4495-b31a-041e61ed5bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beffa844d78142c2a135c1898ff834ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training set and regression plane\n",
    "plot_data_reg_plane(X_train, y_train, w, b, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b2163386-05de-45f0-940d-7e8ea5ff7eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8526.753129079561, 7872.476713596308)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test MSE\n",
    "(mean_squared_error(y_train, hypo(X_train, w, b)),\n",
    " mean_squared_error(y_test, hypo(X_test, w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b65a26a1-180c-419d-aec1-13d10b6445c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6247839415429167, 0.5652616714991141)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test R2\n",
    "r2_score(y_train, hypo(X_train, w, b)), r2_score(y_test, hypo(X_test, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601dd22-4068-43d0-a3ef-8350d7bfaab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
