{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168f6a0a-f1b0-47cd-9b2d-bc3257210301",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca8a54-96b3-4c91-b266-c8960885d6b3",
   "metadata": {},
   "source": [
    "# Multivariate Regression\n",
    "\n",
    "We have a given dataset \n",
    "\n",
    "$$\n",
    "    (X, y) \\in \\mathbb{R}^{m \\times (n+1)},\n",
    "$$\n",
    "\n",
    "where $m$ is the number of samples and $n$ is the number of features.\n",
    "\n",
    "In the dateset we have the features\n",
    "\n",
    "$$\n",
    "    X \\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "\n",
    "and the targets \n",
    "\n",
    "$$\n",
    "    y \\in \\mathbb{R}^m.\n",
    "$$\n",
    "\n",
    "We say that \n",
    "\n",
    "$$\n",
    "    (X^{(i)}, y^{(i)}) \\in \\mathbb{R}^{1 \\times (n+1)}\n",
    "$$\n",
    "\n",
    "is the $i$-th example ($i = 1, \\dots, m$) and \n",
    "\n",
    "$$\n",
    "    X_j \\in \\mathbb{R}^m\n",
    "$$ \n",
    "\n",
    "is the $j$-th feature vector ($j = 1, \\dots, n$), such that \n",
    "\n",
    "$$\n",
    "    X^{(i)}_j \\in \\mathbb{R}  \n",
    "$$\n",
    "\n",
    "is the $j$-th feature of the $i$-th example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf500d-29aa-4eaf-aed5-9e884dcf2d44",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ce5ce3-57c9-49bf-ad09-006a409ac76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_3d(X, y, theta=None, poly_degree=1):\n",
    "    \"\"\"Plot 3d regression problem. Optionally add regression plane.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shape (n_samples, 2)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.\n",
    "        theta : ndarray of shape (2,) or shape (), default=None\n",
    "            Weights for the regression plane. If None no regression plane is\n",
    "            plotted.\n",
    "        poly_degree : int, default=1\n",
    "            Polynom degree.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], y)\n",
    "    if theta is not None:\n",
    "        xx = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n",
    "        yy = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)\n",
    "        xx, yy = np.meshgrid(xx, yy)\n",
    "        XX = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "        poly = PolynomialFeatures(poly_degree, include_bias=False)\n",
    "        Z = hypo(poly.fit_transform(XX), theta[1:], theta[0])\n",
    "        ax.plot_surface(xx, yy, Z.reshape(xx.shape), color='r', alpha=.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d187f75-0c63-413d-b323-4b369d3121d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84205a5453af42d3a72efe09b903cbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make simple example\n",
    "X, y = make_regression(n_samples=100, n_features=2, n_informative=2, noise=100,\n",
    "                       random_state=0)\n",
    "# Plot data\n",
    "plot_reg_3d(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d517d8e-31fc-441f-b226-0855d198fce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 2), (30, 2), (70,), (30,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data in training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=True)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006e765-1843-49f1-a107-fa71500dae3e",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "The hypothesis is given by\n",
    "\n",
    "$$\n",
    "    h_{w, b}(X) = \\sum_{j=1}^n w_j X_j + b = w_1 X_1 + w_2 X_2 + \\dots + w_n X_n + b, \n",
    "$$\n",
    "\n",
    "where $w_1, w_2, \\dots, w_n \\in \\mathbb{R}^n$, with $n$ the number of features, are the weights and $b \\in \\mathbb{R}$ is the bias.\n",
    "\n",
    "We can also write\n",
    "\n",
    "$$\n",
    "    h_{w}(X) = \\sum_{j=0}^n w_j X_j = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_n X_n, \n",
    "$$\n",
    "\n",
    "if we set $w_0 := b$ and $X_0 := 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc707d5-f618-4897-b25f-6d533e940026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the hypothesis and return as shape (n_samples,)\n",
    "hypo = lambda X, w, b: np.sum(w * X, axis=1) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811619a1-52fc-4096-b607-7afb9d824e55",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "The Mean Squared Error is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    MSE(w,b) &= \\frac{1}{m} \\sum\\limits_{i=1}^m (y^{(i)} - h_{w, b}(X^{(i)}))^2\\\\\n",
    "             &= \\frac{1}{m} \\sum\\limits_{i=1}^m (y^{(i)} - \\sum_{j=1}^n w_j X_j^{(i)} + b)^2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of samples and $n$ is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ce523-a217-4365-8998-479e68a01244",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "#### Goal: \n",
    "\n",
    "$$\n",
    "    \\underset{w \\in \\mathbb{R}, ~ b \\in \\mathbb{R}}{\\textbf{minimize}} MSE(w,b)\n",
    "$$\n",
    "\n",
    "#### Idea: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    w &= w - \\alpha \\cdot \\frac{\\partial}{\\partial w} MSE(w,b) \\\\\n",
    "    b &= b - \\alpha \\cdot \\frac{\\partial}{\\partial b} MSE(w,b)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Partial derivaties:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial w} MSE(w,b) &= \\frac{2}{m} \\sum\\limits_{i=1}^m x^{(i)} (y^{(i)} - h_{w,b}(X^{(i)})) \\\\\n",
    "    \\frac{\\partial}{\\partial b} MSE(w,b) &= \\frac{2}{m} \\sum\\limits_{i=1}^m y^{(i)} - h_{w,b}(X^{(i)})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdd0eda-738e-4243-8b63-8560c83b3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, b, alpha, num_iters):  \n",
    "    \"\"\"Simple gradient descent.\n",
    "    \n",
    "    TODO: Plot cost function.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.\n",
    "        w : float\n",
    "            Weight.\n",
    "        b : float\n",
    "            Bias.\n",
    "        alpha : float\n",
    "            Stepsize.\n",
    "        num_iter : int\n",
    "            Number of iterations.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        w : float\n",
    "            Updated weight.\n",
    "        b : float\n",
    "            Updated bias.\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    m = len(X)\n",
    "    \n",
    "    # Iteratively update the weight and bias\n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # Compute predictions (for all samples)\n",
    "        predictions = hypo(X, w, b)\n",
    "        \n",
    "        # Compute residuals (for all samples)\n",
    "        residuals = predictions - y\n",
    "        \n",
    "        # Compute partial derivitaves \n",
    "        w_gradient = 2 * np.mean(residuals.reshape(-1, 1) * X, axis=0)\n",
    "        b_gradient = 2 * np.mean(residuals)\n",
    "\n",
    "        # Update weight and bias\n",
    "        w = w - alpha * w_gradient\n",
    "        b = b - alpha * b_gradient\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66da597c-0082-41cb-862d-84e526fa0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, alpha, num_iters):\n",
    "    \"\"\"Linear regression using gradient descent.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shpae (n_samples, n_features)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.        \n",
    "        alpha : float\n",
    "            Stepsize.\n",
    "        num_iter : int\n",
    "            Number of iterations.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        w : float\n",
    "            Updated weight.\n",
    "        b : float\n",
    "            Updated bias.\n",
    "    \"\"\"\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0.0\n",
    "    w, b = gradient_descent(X, y, w, b, alpha, num_iters)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7529c69-8d4e-4d26-ab63-3f7fbdd8467d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13.89911192, 96.48599232]), 2.14099390223388)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply linear regression with gradient descent\n",
    "w, b = linear_regression(X_train, y_train, 0.0001, 10000)\n",
    "theta = np.concatenate([[b], w])\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5138b44a-0761-4994-9c92-0e367d20926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97836cd3d73d4caa93f2f7f3850ed5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training set and regression plane\n",
    "plot_reg_3d(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9625502-9efe-42c1-b535-17643a35a76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815033e4bba44374952bcf324c096bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot test set and regression plane\n",
    "plot_reg_3d(X_test, y_test, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b49138-5d22-4ae4-8d9d-6cbca34f48ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6992.743228798179, 14196.71422917607)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test MSE\n",
    "(mean_squared_error(y_train, hypo(X_train, w, b)),\n",
    " mean_squared_error(y_test, hypo(X_test, w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7308ee37-ad35-40ea-90f4-d01db238634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6524664292520834, 0.3939626456684733)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test R2\n",
    "r2_score(y_train, hypo(X_train, w, b)), r2_score(y_test, hypo(X_test, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b3471-87d8-4c93-9324-512edd78081e",
   "metadata": {},
   "source": [
    "### Normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c140e0-e47c-4f01-9ef0-1b4ab89a850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    \"\"\"Normal equation for linear regression.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Features.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels.\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "        theta : ndarray of shape (n_features + 1,)\n",
    "            Weight(s) and bias.\n",
    "    \"\"\"\n",
    "    X_hat = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    X_hat_trans = np.transpose(X_hat)\n",
    "    theta = np.linalg.inv(np.dot(X_hat_trans, X_hat))\n",
    "    theta = np.dot(np.dot(theta, X_hat_trans), y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19d19c7d-930b-43cc-8c0b-27b03eecedf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 18.03914973, 108.23897766]), -0.30764114094373207)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the normal equation\n",
    "theta = normal_equation(X_train, y_train)\n",
    "w, b = theta[1:], theta[0]\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f190c52-35b2-418a-90b9-6b53f5fdd96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422ec7c119974a0c86e709b5431b688c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training set and regression plane\n",
    "plot_reg_3d(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ce27a0a-fb3d-45e8-a951-5bb680d0c698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe01266987a46ac914cfad22d0c01a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot test set and regression plane\n",
    "plot_reg_3d(X_test, y_test, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a7dda9-c9b9-4fcc-8983-8a3f18e84b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6824.376355546086, 13924.170927444296)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test MSE\n",
    "(mean_squared_error(y_train, hypo(X_train, w, b)),\n",
    " mean_squared_error(y_test, hypo(X_test, w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89330213-7b70-43c1-b676-1f72e632b78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6608341239810972, 0.4055971280462938)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test R2\n",
    "r2_score(y_train, hypo(X_train, w, b)), r2_score(y_test, hypo(X_test, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2786dc-f167-4119-91ab-f63dbbbc5723",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "TODO: Update\n",
    "\n",
    "The hypothesis for (univariate) *polynomial regression* is given by\n",
    "\n",
    "$$\n",
    "    h_{w, b}(X) = \\sum_{i=1}^h w_i X_i^i + b =  w_1 X + w_2 X^2 + \\dots + w_h X^h + b, \n",
    "$$\n",
    "\n",
    "where $h$ is the degree of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceda1b24-ddc5-4cdb-944a-2a4db54bac7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.51025593e-01,  8.56830612e-01,  4.23834323e-01,\n",
       "        -5.57818657e-01,  7.34158697e-01],\n",
       "       [-6.89549778e-01, -8.03409664e-01,  4.75478896e-01,\n",
       "         5.53990955e-01,  6.45467088e-01],\n",
       "       [ 4.00157208e-01,  1.76405235e+00,  1.60125791e-01,\n",
       "         7.05898262e-01,  3.11188068e+00],\n",
       "       [-1.79924836e-01,  1.17877957e+00,  3.23729465e-02,\n",
       "        -2.12091721e-01,  1.38952128e+00],\n",
       "       [-1.87183850e-01,  4.57585173e-02,  3.50377937e-02,\n",
       "        -8.56525544e-03,  2.09384191e-03]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(2, include_bias=False).fit(X_train)\n",
    "X_train = poly.transform(X_train)\n",
    "X_test = poly.transform(X_test)\n",
    "\n",
    "# Show some samples\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1aa6159-6390-4265-b6f3-a5940ea01875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 15.11875246, 100.88800819,  -5.96802409,  -1.86176959,\n",
       "         18.95112233]),\n",
       " -14.347459224554276)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train linear regression\n",
    "theta = normal_equation(X_train, y_train)\n",
    "w, b = theta[1:], theta[0]\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c73bdf27-0f4d-4404-9bda-cfe921686b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d4c3a4588a41d9b471693d96bee69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training set and regression plane\n",
    "plot_reg_3d(X_train, y_train, theta, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37ab5f93-d7ef-4495-b31a-041e61ed5bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a26a0627064153bc211038dcef52a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training set and regression plane\n",
    "plot_reg_3d(X_train, y_train, theta, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2163386-05de-45f0-940d-7e8ea5ff7eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6111.20370302288, 13365.99308990835)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test MSE\n",
    "(mean_squared_error(y_train, hypo(X_train, w, b)),\n",
    " mean_squared_error(y_test, hypo(X_test, w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b65a26a1-180c-419d-aec1-13d10b6445c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6962782165756068, 0.4294249387950356)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training and test R2\n",
    "r2_score(y_train, hypo(X_train, w, b)), r2_score(y_test, hypo(X_test, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601dd22-4068-43d0-a3ef-8350d7bfaab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
